{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342, 14)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv('../data/train_labels.csv')\n",
    "train_metadata = pd.read_csv('../data/train_metadata.csv')\n",
    "\n",
    "train = train_metadata.merge(train_labels, on='filename', how='inner')\n",
    "\n",
    "def process_age(age_str):\n",
    "    age_str = age_str.replace('[', '').replace(']', '').split(':')\n",
    "    return (int(age_str[0]) + int(age_str[1])) / 2\n",
    "\n",
    "body_map = ['thigh', 'trunc', 'face', 'forearm', 'arm', 'leg', 'hand', 'foot', 'sole', 'finger', 'neck', 'toe', 'seat', 'scalp', 'nail','lower limb/hip', 'hand/foot/nail', 'head/neck', 'upper limb/shoulder', 'other']\n",
    "def process_body(body):\n",
    "    if body in ['thigh', 'trunc', 'face', 'forearm', 'arm', 'leg', 'hand', 'foot', 'sole', 'finger', 'neck', 'toe', 'seat', 'scalp', 'nail']:\n",
    "        return body_map.index(body)\n",
    "    else:\n",
    "        return body_map.index('other')\n",
    "   \n",
    "mel_map = {'other': 0, 'YES': 1, 'NO': 2}\n",
    "\n",
    "train.age = train.age.apply(lambda x: process_age(x)).astype(int)\n",
    "train.body_site = train.body_site.replace('trunk', 'trunc') .fillna('other').apply(process_body).astype(int)\n",
    "train.melanoma_history = train.melanoma_history.fillna('other').apply(lambda x: mel_map[x]).astype(int)\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisioMel_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        self.filenames, self.y = data['filename'], data['relapse']\n",
    "        \n",
    "        self.age = data['age'] / 100\n",
    "        self.sex = data['sex'] - 1\n",
    "        self.body_site = data['body_site']\n",
    "        self.melanoma_history = data['melanoma_history']\n",
    "        \n",
    "        self.mean, self.std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "        self.eval_t = transforms.Compose([transforms.ToTensor(), \n",
    "                                          transforms.Normalize(mean = self.mean, std = self.std)#,\n",
    "                                          #transforms.Resize((224,224))\n",
    "                                        ])\n",
    "        \n",
    "        self.labels = torch.tensor(self.y.values, dtype = torch.float32)\n",
    "        \n",
    "        # To tensor labels\n",
    "        self.age = torch.tensor(self.age.values, dtype = torch.float32)\n",
    "        self.sex = torch.tensor(self.sex.values, dtype = torch.long)\n",
    "        self.body_site = torch.tensor(self.body_site.values, dtype = torch.long)\n",
    "        self.melanoma_history= torch.tensor(self.melanoma_history.values, dtype = torch.long)\n",
    "        \n",
    "        print(f'{self.labels.shape}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open('../data/images/'+self.filenames[index])\n",
    "        image = self.eval_t(image)\n",
    "        return image, self.age[index], self.sex[index], self.body_site[index], self.melanoma_history[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1342])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nvalid_dataloader = DataLoader(\\n            valid_dataset,  \\n            sampler = SequentialSampler(valid_dataset),\\n            batch_size = 1\\n        )\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = VisioMel_Dataset(train)\n",
    "#valid_dataset = VisioMel_Dataset(test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "                train_dataset,  \n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size = 1\n",
    "            )\n",
    "'''\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_dataset,  \n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size = 1\n",
    "        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;sys.path.append('../src/HIPT_4K')\n",
    "from hipt_4k import HIPT_4K\n",
    "from hipt_model_utils import get_vit256, get_vit4k\n",
    "\n",
    "class VisioMel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        pretrained_weights256 = '../src/HIPT_4K/Checkpoints/vit256_small_dino.pth'\n",
    "        pretrained_weights4k = '../src/HIPT_4K/Checkpoints/vit4k_xs_dino.pth'\n",
    "        self.device256 = torch.device('cuda:0')\n",
    "        self.device4k = torch.device('cuda:0')\n",
    "\n",
    "        ### ViT_256 + ViT_4K loaded into HIPT_4K API\n",
    "        self.backbone = HIPT_4K(pretrained_weights256, pretrained_weights4k, self.device256, self.device4k)\n",
    "        \n",
    "        self.body = nn.Embedding(20, 11)\n",
    "        self.sex = nn.Embedding(2, 3)\n",
    "        self.mel = nn.Embedding(3, 5)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        self.cls1 = nn.Linear(212, 40)\n",
    "        #self.cls2 = nn.Linear(100, 35)\n",
    "        self.cls3 = nn.Linear(40, 10)\n",
    "        self.cls4 = nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "    def forward(self, img, xage ,xsex, xbody, xmel):\n",
    "\n",
    "        x = self.backbone(img) #.forward(x)\n",
    "        x2 = self.sex(xsex)\n",
    "        x3 = self.body(xbody)\n",
    "        x4 = self.mel(xmel)\n",
    "            \n",
    "        x = torch.cat([x, xage, x2, x3, x4], dim=1)\n",
    "        \n",
    "        x = self.act(x)\n",
    "        x = self.cls1(x)\n",
    "        x = self.act(x)\n",
    "        #x = self.cls2(x)\n",
    "        #x = self.act(x)\n",
    "        x = self.cls3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.cls4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ../src/HIPT_4K/Checkpoints/vit256_small_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "# of Patches: 196\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ../src/HIPT_4K/Checkpoints/vit4k_xs_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisioMel(\n",
       "  (backbone): HIPT_4K(\n",
       "    (model256): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (model4k): VisionTransformer4K(\n",
       "      (phi): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (body): Embedding(20, 11)\n",
       "  (sex): Embedding(2, 3)\n",
       "  (mel): Embedding(3, 5)\n",
       "  (act): ReLU()\n",
       "  (cls1): Linear(in_features=212, out_features=40, bias=True)\n",
       "  (cls3): Linear(in_features=40, out_features=10, bias=True)\n",
       "  (cls4): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "#model = VisioMel()\n",
    "#model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "model = VisioMel().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('../artifacts/ckpt_full_5.pth')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "optimizer.load_state_dict(ckpt['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1342/1342 [32:11<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.34846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [32:11<10728:40:50, 1931.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1342/1342 [31:15<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.34206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/20000 [1:03:26<10545:50:53, 1898.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1342/1342 [31:14<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.34039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/20000 [1:34:41<10486:28:52, 1887.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1184/1342 [27:28<03:39,  1.39s/it]\n",
      "  0%|          | 3/20000 [2:02:10<13572:45:28, 2443.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_dataloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader)):\n\u001b[0;32m     13\u001b[0m     b_img \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device) \u001b[39m# xage ,xsex, xbody, xmel\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     b_age \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mVisioMel_Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     34\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39m../data/images/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilenames[index])\n\u001b[1;32m---> 35\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_t(image)\n\u001b[0;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m image, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mage[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msex[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody_site[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmelanoma_history[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index]\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    165\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 166\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\PIL\\Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 701\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[0;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (\u001b[39mMemoryError\u001b[39;00m, \u001b[39mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\PIL\\Image.py:758\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[0;32m    756\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m--> 758\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    760\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    761\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:1196\u001b[0m, in \u001b[0;36mTiffImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1195\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtile \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_load_libtiff:\n\u001b[1;32m-> 1196\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_libtiff()\n\u001b[0;32m   1197\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mload()\n",
      "File \u001b[1;32mc:\\Users\\vicallejasfuentes\\AppData\\Local\\miniconda3\\envs\\myenv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:1294\u001b[0m, in \u001b[0;36mTiffImageFile._load_libtiff\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1292\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   1293\u001b[0m     \u001b[39m# 4 bytes, otherwise the trace might error out\u001b[39;00m\n\u001b[1;32m-> 1294\u001b[0m     n, err \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfpfp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1296\u001b[0m     \u001b[39m# we have something else.\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mdon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have fileno or getvalue. just reading\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs,leave=True) as pbar:\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        print(f'Epoch {epoch_i}')\n",
    "\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "\n",
    "            b_img = batch[0].to(device) # xage ,xsex, xbody, xmel\n",
    "            b_age = batch[1].unsqueeze(0).to(device)\n",
    "            b_sex = batch[2].to(device)\n",
    "            b_body = batch[3].to(device)\n",
    "            b_mel = batch[4].to(device)\n",
    "            b_labels = batch[5].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                b_logits = model(b_img, b_age, b_sex, b_body, b_mel).squeeze(-1)\n",
    "                    \n",
    "            loss = criterion(b_logits,b_labels)\n",
    "            loss.backward()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_train_loss = (total_train_loss/len(train_dataloader))\n",
    "        \n",
    "        print(f'Train: loss {avg_train_loss:.5f}')\n",
    "        '''\n",
    "        model.eval()\n",
    "\n",
    "        total_dev_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(valid_dataloader):\n",
    "\n",
    "            b_img = batch[0].to(device) # xage ,xsex, xbody, xmel\n",
    "            b_age = batch[1].unsqueeze(0).to(device)\n",
    "            b_sex = batch[2].to(device)\n",
    "            b_body = batch[3].to(device)\n",
    "            b_mel = batch[4].to(device)\n",
    "            b_labels = batch[5].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                with torch.no_grad(): \n",
    "                    b_logits = model(b_img, b_age, b_sex, b_body, b_mel).squeeze(-1)\n",
    "\n",
    "            loss = criterion(b_logits,b_labels)\n",
    "            total_dev_loss += loss.item()\n",
    "        \n",
    "        avg_dev_loss = (total_dev_loss/len(valid_dataloader))\n",
    "s\n",
    "        print(f'Dev: loss {avg_dev_loss:.5f}')\n",
    "        '''\n",
    "        torch.save({\n",
    "            'epoch': epoch_i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'../artifacts/ckpt_full_{epoch_i}.pth')\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
