{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1342, 14)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv('../data/train_labels.csv')\n",
    "train_metadata = pd.read_csv('../data/train_metadata.csv')\n",
    "\n",
    "train = train_metadata.merge(train_labels, on='filename', how='inner')\n",
    "\n",
    "def process_age(age_str):\n",
    "    age_str = age_str.replace('[', '').replace(']', '').split(':')\n",
    "    return (int(age_str[0]) + int(age_str[1])) / 2\n",
    "\n",
    "body_map = ['thigh', 'trunc', 'face', 'forearm', 'arm', 'leg', 'hand', 'foot', 'sole', 'finger', 'neck', 'toe', 'seat', 'scalp', 'nail','lower limb/hip', 'hand/foot/nail', 'head/neck', 'upper limb/shoulder', 'other']\n",
    "def process_body(body):\n",
    "    if body in ['thigh', 'trunc', 'face', 'forearm', 'arm', 'leg', 'hand', 'foot', 'sole', 'finger', 'neck', 'toe', 'seat', 'scalp', 'nail']:\n",
    "        return body_map.index(body)\n",
    "    else:\n",
    "        return body_map.index('other')\n",
    "   \n",
    "mel_map = {'other': 0, 'YES': 1, 'NO': 2}\n",
    "\n",
    "train.age = train.age.apply(lambda x: process_age(x)).astype(int)\n",
    "train.body_site = train.body_site.replace('trunk', 'trunc') .fillna('other').apply(process_body).astype(int)\n",
    "train.melanoma_history = train.melanoma_history.fillna('other').apply(lambda x: mel_map[x]).astype(int)\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisioMel_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        self.filenames, self.y = data['filename'], data['relapse']\n",
    "        \n",
    "        self.age = data['age'] / 100\n",
    "        self.sex = data['sex'] - 1\n",
    "        self.body_site = data['body_site']\n",
    "        self.melanoma_history = data['melanoma_history']\n",
    "        \n",
    "        self.mean, self.std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "        self.eval_t = transforms.Compose([transforms.ToTensor(), \n",
    "                                          transforms.Normalize(mean = self.mean, std = self.std)#,\n",
    "                                          #transforms.Resize((224,224))\n",
    "                                        ])\n",
    "        \n",
    "        self.labels = torch.tensor(self.y.values, dtype = torch.float32)\n",
    "        \n",
    "        # To tensor labels\n",
    "        self.age = torch.tensor(self.age.values, dtype = torch.float32)\n",
    "        self.sex = torch.tensor(self.sex.values, dtype = torch.long)\n",
    "        self.body_site = torch.tensor(self.body_site.values, dtype = torch.long)\n",
    "        self.melanoma_history= torch.tensor(self.melanoma_history.values, dtype = torch.long)\n",
    "        \n",
    "        print(f'{self.labels.shape}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open('../data/images/'+self.filenames[index])\n",
    "        image = self.eval_t(image)\n",
    "        return image, self.age[index], self.sex[index], self.body_site[index], self.melanoma_history[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1073])\n",
      "torch.Size([269])\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = VisioMel_Dataset(train)\n",
    "valid_dataset = VisioMel_Dataset(test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "                train_dataset,  \n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size = 1\n",
    "            )\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_dataset,  \n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size = 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;sys.path.append('../src/HIPT_4K')\n",
    "from hipt_4k import HIPT_4K\n",
    "from hipt_model_utils import get_vit256, get_vit4k\n",
    "\n",
    "class VisioMel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        pretrained_weights256 = '../src/HIPT_4K/Checkpoints/vit256_small_dino.pth'\n",
    "        pretrained_weights4k = '../src/HIPT_4K/Checkpoints/vit4k_xs_dino.pth'\n",
    "        self.device256 = torch.device('cuda:1')\n",
    "        self.device4k = torch.device('cuda:1')\n",
    "\n",
    "        ### ViT_256 + ViT_4K loaded into HIPT_4K API\n",
    "        self.backbone = HIPT_4K(pretrained_weights256, pretrained_weights4k, self.device256, self.device4k)\n",
    "        \n",
    "        self.body = nn.Embedding(20, 11)\n",
    "        self.sex = nn.Embedding(2, 3)\n",
    "        self.mel = nn.Embedding(3, 5)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        self.cls1 = nn.Linear(212, 40)\n",
    "        #self.cls2 = nn.Linear(100, 35)\n",
    "        self.cls3 = nn.Linear(40, 10)\n",
    "        self.cls4 = nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "    def forward(self, img, xage ,xsex, xbody, xmel):\n",
    "\n",
    "        x = self.backbone(img) #.forward(x)\n",
    "        x2 = self.sex(xsex)\n",
    "        x3 = self.body(xbody)\n",
    "        x4 = self.mel(xmel)\n",
    "            \n",
    "        x = torch.cat([x, xage, x2, x3, x4], dim=1)\n",
    "        \n",
    "        x = self.act(x)\n",
    "        x = self.cls1(x)\n",
    "        x = self.act(x)\n",
    "        #x = self.cls2(x)\n",
    "        #x = self.act(x)\n",
    "        x = self.cls3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.cls4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ../src/HIPT_4K/Checkpoints/vit256_small_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "# of Patches: 196\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ../src/HIPT_4K/Checkpoints/vit4k_xs_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisioMel(\n",
       "  (backbone): HIPT_4K(\n",
       "    (model256): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (model4k): VisionTransformer4K(\n",
       "      (phi): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (body): Embedding(20, 11)\n",
       "  (sex): Embedding(2, 3)\n",
       "  (mel): Embedding(3, 5)\n",
       "  (act): ReLU()\n",
       "  (cls1): Linear(in_features=212, out_features=40, bias=True)\n",
       "  (cls3): Linear(in_features=40, out_features=10, bias=True)\n",
       "  (cls4): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "#model = VisioMel()\n",
    "#model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "model = VisioMel().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('../artifacts/ckpt.pth')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "optimizer.load_state_dict(ckpt['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1073/1073 [25:18<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.36555\n",
      "Dev: loss 0.33912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [31:25<10475:35:33, 1885.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1073/1073 [24:44<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.36027\n",
      "Dev: loss 0.33569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/20000 [1:02:17<10363:24:22, 1865.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1073/1073 [24:41<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.34873\n",
      "Dev: loss 0.37653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/20000 [1:33:04<10317:14:15, 1857.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1073/1073 [24:41<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.34988\n",
      "Dev: loss 0.39459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/20000 [2:03:51<10292:08:56, 1852.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1073 [00:30<24:03,  1.37s/it]\n",
      "  0%|          | 4/20000 [2:04:21<10360:44:03, 1865.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_dataloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader)):\n\u001b[1;32m---> 13\u001b[0m     b_img \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device) \u001b[39m# xage ,xsex, xbody, xmel\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     b_age \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m     b_sex \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs,leave=True) as pbar:\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        print(f'Epoch {epoch_i}')\n",
    "\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "\n",
    "            b_img = batch[0].to(device) # xage ,xsex, xbody, xmel\n",
    "            b_age = batch[1].unsqueeze(0).to(device)\n",
    "            b_sex = batch[2].to(device)\n",
    "            b_body = batch[3].to(device)\n",
    "            b_mel = batch[4].to(device)\n",
    "            b_labels = batch[5].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                b_logits = model(b_img, b_age, b_sex, b_body, b_mel).squeeze(-1)\n",
    "                    \n",
    "            loss = criterion(b_logits,b_labels)\n",
    "            loss.backward()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_train_loss = (total_train_loss/len(train_dataloader))\n",
    "        \n",
    "        print(f'Train: loss {avg_train_loss:.5f}')\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_dev_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(valid_dataloader):\n",
    "\n",
    "            b_img = batch[0].to(device) # xage ,xsex, xbody, xmel\n",
    "            b_age = batch[1].unsqueeze(0).to(device)\n",
    "            b_sex = batch[2].to(device)\n",
    "            b_body = batch[3].to(device)\n",
    "            b_mel = batch[4].to(device)\n",
    "            b_labels = batch[5].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                with torch.no_grad(): \n",
    "                    b_logits = model(b_img, b_age, b_sex, b_body, b_mel).squeeze(-1)\n",
    "\n",
    "            loss = criterion(b_logits,b_labels)\n",
    "            total_dev_loss += loss.item()\n",
    "        \n",
    "        avg_dev_loss = (total_dev_loss/len(valid_dataloader))\n",
    "\n",
    "        print(f'Dev: loss {avg_dev_loss:.5f}')\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, '../artifacts/ckpt.pth')\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
